  virtual void ExtendMklDnnMatMulFwdParams(OpKernelContext* context,
                                           MklDnnMatMulFwdParams& params) {
    // Append data type names of input, weight, bias, and output.
    params.dtypes.append(typeid(Tinput).name());
    params.dtypes.append(typeid(Tweight).name());
    params.dtypes.append(typeid(Tbias).name());
    params.dtypes.append(typeid(Toutput).name());

    // min-max values for input and weight should be scalar.
    const Tensor& min_input_tensor = context->input(3);
    const Tensor& max_input_tensor = context->input(4);
    const Tensor& min_weight_tensor = context->input(5);
    const Tensor& max_weight_tensor = context->input(6);

    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_input_tensor.shape()),
                errors::InvalidArgument("`min_a` must be rank 0 but is rank ",
                                        min_input_tensor.dims()));
    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_input_tensor.shape()),
                errors::InvalidArgument("`max_a` must be rank 0 but is rank ",
                                        max_input_tensor.dims()));
    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_weight_tensor.shape()),
                errors::InvalidArgument("`min_b` must be rank 0 but is rank ",
                                        min_weight_tensor.dims()));
    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_weight_tensor.shape()),
                errors::InvalidArgument("`max_b` must be rank 0 but is rank ",
                                        max_weight_tensor.dims()));

    // When the output type is quint8, the output data is requantized into
    // quint8. A post_op "output_scale" is added to do the conversion.
    if (std::is_same<Toutput, quint8>::value ||
        std::is_same<Toutput, qint8>::value ||
        std::is_same<Toutput, float>::value) {
      float min_output_value;
      float max_output_value;
      ComputeOutputRangeForInt32(context, &min_output_value, &max_output_value);
      float scale_int32 =
          std::max(std::abs(min_output_value), std::abs(max_output_value));
      const Tensor& min_freezed_tensor = context->input(7);
      const Tensor& max_freezed_tensor = context->input(8);
      // min-max values of freezed output range should be scalar.
      OP_REQUIRES(context,
                  TensorShapeUtils::IsScalar(min_freezed_tensor.shape()),
                  errors::InvalidArgument(
                      "`min_freezed_output` must be rank 0 but is rank ",
                      min_freezed_tensor.dims()));
      OP_REQUIRES(context,
                  TensorShapeUtils::IsScalar(max_freezed_tensor.shape()),
                  errors::InvalidArgument(
                      "`max_freezed_output` must be rank 0 but is rank ",
                      max_freezed_tensor.dims()));
      const float min_freezed_output = min_freezed_tensor.scalar<float>()();
      const float max_freezed_output = max_freezed_tensor.scalar<float>()();
      float scale_eightbit =
          std::max(std::abs(min_freezed_output), std::abs(max_freezed_output));
      float scale = 1.0;
      if (std::is_same<Toutput, quint8>::value) {
        scale = scale_int32 / scale_eightbit / static_cast<float>(1u << 23);
      } else if (std::is_same<Toutput, qint8>::value) {
        scale = scale_int32 / scale_eightbit / static_cast<float>(1u << 24);
      } else if (std::is_same<Toutput, float>::value) {
        scale = scale_int32 / static_cast<float>(1u << 31);
      } else {
        // TODO(intel-tf): Keep the default qint8 as before.
        // Change to error later.
        scale = scale_int32 / scale_eightbit / static_cast<float>(1u << 24);
      }
      std::vector<float> output_scale;
      output_scale.push_back(scale);
      params.post_op_params.push_back({"output_scale", output_scale});
    }
  }